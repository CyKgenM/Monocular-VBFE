{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ac99d5-dd14-4496-a67e-62b07637c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "#from plyfile import PlyData\n",
    "from pytorch_tcn import TCN, TemporalConv1d, TemporalConvTranspose1d\n",
    "#from torch_points3d.core import data_transform as trf\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "#from torch_pointnet import PointNetfeat as PNet\n",
    "#from pointnet import get_model as PNet\n",
    "#from pointnet2 import get_model as PNet2\n",
    "from pointnet2_ssg import get_model as PNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469b890a-cc85-4a4d-847f-b696815de667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86161c7c-a3a9-4dc5-b84c-308b0b94cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ply(path):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    #offset_tensor = torch.tensor([0, 0.325, 0]) # offset between the Point Cloud and RGB\n",
    "    downsampled_pc = pcd.farthest_point_down_sample(2048)\n",
    "    \n",
    "    pos = np.asarray(downsampled_pc.points)\n",
    "    pos = torch.tensor(pos, dtype=torch.float32)\n",
    "    #pos += offset_tensor\n",
    "    \n",
    "    # Return point cloud data as PyG Data object (no faces for point clouds)\n",
    "    data = Data(pos=pos)\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_image_number(filename):\n",
    "    match = re.search(r'frame_(\\d+)\\.png', filename)\n",
    "    return int(match.group(1)) if match else -1  # Extract number, fallback -1 for safety\n",
    "\n",
    "def extract_pc_number(filename):\n",
    "    match = re.search(r'frame_(\\d+)\\.ply', filename)\n",
    "    return int(match.group(1)) if match else -1  # Extract number, fallback -1 for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2258529d-782d-4ae6-86d5-301a57d4cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rgb(rgb_image):\n",
    "    # Convert to numpy array and transpose for display (from CxHxW to HxWxC)\n",
    "    rgb_image_np = rgb_image.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Plot the RGB image using matplotlib\n",
    "    plt.imshow(rgb_image_np)\n",
    "    plt.title(\"RGB Image\")\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "\n",
    "# Step 2: Visualize the Point Cloud\n",
    "def visualize_point_cloud(point_cloud):\n",
    "    # Convert the point cloud tensor to numpy array\n",
    "    point_cloud_np = point_cloud.numpy()\n",
    "\n",
    "    # Create an Open3D point cloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    \n",
    "    # Convert the point cloud array to Open3D format\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud_np)\n",
    "\n",
    "    # Create a visualizer and add the point cloud\n",
    "    o3d.visualization.draw_geometries([pcd], window_name=\"Translated Point Cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4265a57c-ac37-4f5e-aecb-a496ed1ce59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_17116\\1278133784.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"pnet2_weights_ssg.pth\")\n",
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_17116\\1278133784.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"pnet2_weights_ssg.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor batch in train_dataloader:\\n    cats, labels = batch  # Unpack all three: images, pointclouds, and labels\\n    print(\"Cats batch size:\", cats.size())\\n    #print(\"Pointcloud batch size:\", pointclouds.size())\\n    print(\"Labels:\", labels)\\n    break\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RPC_Dataset(Dataset):\n",
    "    def __init__(self, image_dir, pointcloud_dir, forces, image_transform=None, pointcloud_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.pointcloud_dir = pointcloud_dir\n",
    "        self.labels = pd.read_csv(forces)\n",
    "\n",
    "        rgb_transform = v2.Compose([\n",
    "            v2.Resize((224, 224)),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "        self.image_transform = rgb_transform\n",
    "        self.image_augment = image_transform\n",
    "        self.pointcloud_transform = pointcloud_transform\n",
    "\n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        vgg.eval()\n",
    "        vgg.to(device)\n",
    "        self.rgb_feature_extractor = nn.Sequential(\n",
    "            *list(vgg.features.children()),   # Convolutional layers\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),     # Ensure 7x7 output size\n",
    "            nn.Flatten(),                     # Flatten to (batch_size, 25088)\n",
    "            *list(vgg.classifier.children())[:-1]  # Use up to the second to last FC layer (4096 output)\n",
    "        )\n",
    "        \n",
    "        pointnet = PNet2(40, normal_channel=False)\n",
    "        #checkpoint = torch.load(\"pnet2_weights.pth\")\n",
    "        checkpoint = torch.load(\"pnet2_weights_ssg.pth\")\n",
    "        pointnet.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        pointnet.eval()\n",
    "        pointnet.to(device)\n",
    "        self.pc_feature_extractor = pointnet\n",
    "        '''\n",
    "        self.pc_feature_extractor = nn.Sequential(\n",
    "            *list(pointnet.children())[:-1],\n",
    "            nn.Linear(40, 512)\n",
    "        )\n",
    "        '''\n",
    "        self.seq_length = 15\n",
    "\n",
    "        # Get a list of image and point cloud file names (assuming they match)\n",
    "        self.image_filenames = sorted(os.listdir(image_dir), key=extract_image_number)\n",
    "        self.pointcloud_filenames = sorted(os.listdir(pointcloud_dir), key=extract_pc_number)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print(len(self.image_filenames))\n",
    "        return int(len(self.image_filenames) / self.seq_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        if idx >= len(self.labels):\n",
    "            raise IndexError(f\"Index {idx} is out of bounds for labels of size {len(self.labels)}\")\n",
    "        \n",
    "        cat_tensors = []\n",
    "        #print(self.pc_feature_extractor)\n",
    "        for i in range(self.seq_length):\n",
    "            img_path = os.path.join(self.image_dir, self.image_filenames[idx + i])\n",
    "            image = read_image(img_path, ImageReadMode.RGB)\n",
    "            # Apply base transform\n",
    "            image = self.image_transform(image)\n",
    "            pointcloud_path = os.path.join(self.pointcloud_dir, self.pointcloud_filenames[idx + i])\n",
    "            pointcloud = read_ply(pointcloud_path)\n",
    "            # Apply augmentation\n",
    "            if self.pointcloud_transform or self.image_augment:\n",
    "                if self.image_augment:\n",
    "                    image = self.image_augment(image)\n",
    "                if self.pointcloud_transform:\n",
    "                    pointcloud = self.pointcloud_transform(pointcloud)\n",
    "            pointcloud_tensor = pointcloud.pos.clone().detach()\n",
    "            image, pointcloud_tensor = image.to(device), pointcloud_tensor.to(device)\n",
    "            pointcloud_tensor = pointcloud_tensor.transpose(0, 1)\n",
    "            image = image.unsqueeze(0)\n",
    "            pointcloud_tensor = pointcloud_tensor.unsqueeze(0)\n",
    "            #print(pointcloud_tensor.size())\n",
    "            rgb_features = self.rgb_feature_extractor(image)\n",
    "            pc_features = self.pc_feature_extractor(pointcloud_tensor)[0]\n",
    "            #print(i)\n",
    "            rgb_features = rgb_features.squeeze(0)\n",
    "            pc_features = pc_features.squeeze(0)\n",
    "            cat = torch.cat((rgb_features, pc_features), dim=0)\n",
    "            # Stack the frames into separate sequences\n",
    "            cat_tensors.append(cat)\n",
    "            \n",
    "        #images = torch.stack(images, dim=0)  # Shape: (seq_length, C, H, W)\n",
    "        #pointclouds = torch.stack(pointclouds, dim=0)  # Shape: (seq_length, N, 3)\n",
    "        cat_tensors = torch.stack(cat_tensors, dim=0)\n",
    "        label = self.labels.iloc[idx + self.seq_length - 2, 1]\n",
    "        contact = self.labels.iloc[idx + self.seq_length - 2, 2]\n",
    "        \n",
    "        return cat_tensors, torch.tensor(label, dtype=torch.float32), torch.tensor(contact, dtype=torch.bool)\n",
    "\n",
    "rgb_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "# Define transform for the images (if needed)\n",
    "rgb_augment = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.AutoAugment()\n",
    "])\n",
    "\n",
    "pc_augment = T.Compose([\n",
    "    T.RandomJitter(0.01),\n",
    "    T.RandomRotate(30),\n",
    "    T.RandomScale((0.8, 1.2))\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "train_image_dir = 'data/train_images'          # Path to folder containing images\n",
    "train_pointcloud_dir = 'data/train_pcs'  # Path to folder containing .ply point clouds\n",
    "#train_forces = 'data/train_labels.csv'\n",
    "train_forces = 'data/updated_train.csv'\n",
    "\n",
    "test_image_dir = 'data/test_images'          # Path to folder containing images\n",
    "test_pointcloud_dir = 'data/test_pcs'  # Path to folder containing .ply point clouds\n",
    "#test_forces = 'data/test_labels.csv'\n",
    "test_forces = 'data/updated_test.csv'\n",
    "\n",
    "train_dataset = RPC_Dataset(train_image_dir, train_pointcloud_dir, train_forces)\n",
    "test_dataset = RPC_Dataset(test_image_dir, test_pointcloud_dir, test_forces)\n",
    "#augmented_test_dataset = RPC_Dataset(test_image_dir, test_pointcloud_dir, test_forces, image_transform=rgb_augment, pointcloud_transform=pc_augment)\n",
    "#augmented_test_dataset = ConcatDataset([test_dataset, augmented_test_dataset])\n",
    "\n",
    "'''\n",
    "data = dataset.__getitem__(5)\n",
    "image, pointcloud = data[0], data[1]\n",
    "visualize_rgb(image)\n",
    "visualize_point_cloud(pointcloud)\n",
    "'''\n",
    "# Use DataLoader to batch and do not shuffle the data to keep it sequential\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "'''\n",
    "for batch in train_dataloader:\n",
    "    cats, labels = batch  # Unpack all three: images, pointclouds, and labels\n",
    "    print(\"Cats batch size:\", cats.size())\n",
    "    #print(\"Pointcloud batch size:\", pointclouds.size())\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5694dae-1a1d-4bc5-9dbf-468ef3f04c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPC_TCN(\n",
      "  (fc1): Linear(in_features=4608, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1280, bias=True)\n",
      "  (fc3): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (fc4): Linear(in_features=1280, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (tconv1): TemporalConv1d(\n",
      "    256, 64, kernel_size=(3,), stride=(1,)\n",
      "    (padder): ConstantPad1d(padding=(2, 0), value=0.0)\n",
      "  )\n",
      "  (tconv2): TemporalConv1d(\n",
      "    64, 128, kernel_size=(3,), stride=(1,)\n",
      "    (padder): ConstantPad1d(padding=(2, 0), value=0.0)\n",
      "  )\n",
      "  (tconv3): TemporalConv1d(\n",
      "    128, 256, kernel_size=(3,), stride=(1,)\n",
      "    (padder): ConstantPad1d(padding=(2, 0), value=0.0)\n",
      "  )\n",
      "  (bnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnorm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tcn): TCN(\n",
      "    (network): ModuleList(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): ParametrizedTemporalConv1d(\n",
      "          4608, 64, kernel_size=(3,), stride=(1,)\n",
      "          (padder): ConstantPad1d(padding=(2, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (conv2): ParametrizedTemporalConv1d(\n",
      "          64, 64, kernel_size=(3,), stride=(1,)\n",
      "          (padder): ConstantPad1d(padding=(2, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (activation1): ReLU()\n",
      "        (activation2): ReLU()\n",
      "        (activation_final): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (downsample): Conv1d(4608, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): ParametrizedTemporalConv1d(\n",
      "          64, 128, kernel_size=(3,), stride=(1,), dilation=(2,)\n",
      "          (padder): ConstantPad1d(padding=(4, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (conv2): ParametrizedTemporalConv1d(\n",
      "          128, 128, kernel_size=(3,), stride=(1,), dilation=(2,)\n",
      "          (padder): ConstantPad1d(padding=(4, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (activation1): ReLU()\n",
      "        (activation2): ReLU()\n",
      "        (activation_final): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (downsample): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): ParametrizedTemporalConv1d(\n",
      "          128, 256, kernel_size=(3,), stride=(1,), dilation=(4,)\n",
      "          (padder): ConstantPad1d(padding=(8, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (conv2): ParametrizedTemporalConv1d(\n",
      "          256, 256, kernel_size=(3,), stride=(1,), dilation=(4,)\n",
      "          (padder): ConstantPad1d(padding=(8, 0), value=0.0)\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (activation1): ReLU()\n",
      "        (activation2): ReLU()\n",
      "        (activation_final): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RPC_TCN(nn.Module):\n",
    "    def __init__(self, input_size=4608, output_size=1, num_channels=[64, 128, 256], kernel_size=3, dropout=0.2):\n",
    "        super(RPC_TCN, self).__init__()\n",
    "        # Spatial Block\n",
    "        \n",
    "        #self.full = nn.Linear(1024, 512)\n",
    "\n",
    "        # Temporal Block\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1280)\n",
    "        self.fc3 = nn.Linear(1280, 1280)\n",
    "        self.fc4 = nn.Linear(1280, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tconv1 = TemporalConv1d(256, 64, kernel_size)\n",
    "        self.tconv2 = TemporalConv1d(64, 128, kernel_size)\n",
    "        self.tconv3 = TemporalConv1d(128, 256, kernel_size)\n",
    "        #self.tconvt1 = TemporalConvTranspose1d(256, 64, kernel_size, stride=2)\n",
    "        #self.tconvt2 = TemporalConvTranspose1d(64, 128, kernel_size, stride=2)\n",
    "        #self.tconvt3 = TemporalConvTranspose1d(128, 256, kernel_size, stride=2)\n",
    "        self.bnorm1 = nn.BatchNorm1d(64)\n",
    "        self.bnorm2 = nn.BatchNorm1d(128)\n",
    "        self.bnorm3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.tcn = TCN(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, cat_input):\n",
    "        \n",
    "        # TB forward prop\n",
    "        #cat = self.fc1(cat)\n",
    "        #print(cat.size())\n",
    "        #cat = F.relu(self.bnorm1(self.tconv1(cat)))\n",
    "        #cat = F.relu(self.bnorm2(self.tconv2(cat)))\n",
    "        #cat = F.relu(self.bnorm3(self.tconv3(cat)))\n",
    "        #cat = F.relu(self.bnorm1(self.tconvt1(cat)))\n",
    "        #cat = F.relu(self.bnorm2(self.tconvt2(cat)))\n",
    "        #cat = F.relu(self.bnorm3(self.tconvt3(cat)))\n",
    "        #force = F.relu(self.fc4(self.fc3(self.fc2(cat))))\n",
    "        cat_input = cat_input.transpose(1, 2)\n",
    "        #print(cat.size())\n",
    "        #print(cat)\n",
    "        force = self.tcn(cat_input)  # Apply TCN layer\n",
    "        #print(force.size())\n",
    "        force = self.fc(force[:, :, -1])  # Take the last time step and pass through fully connected layer\n",
    "        #print(force.size())\n",
    "        return force\n",
    "\n",
    "model = RPC_TCN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00990e5-3797-4b65-8e4b-d03dcedcba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "loss_fn = nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "def contact_loss(pred, f, contact, func='MSE', weight=10):\n",
    "    loss_fn = nn.MSELoss() if func == 'MSE' else nn.L1Loss()\n",
    "    loss = loss_fn(pred, f)\n",
    "\n",
    "    weighted_loss = torch.where(contact == 0, weight*loss, loss)\n",
    "\n",
    "    return torch.mean(weighted_loss)\n",
    "\n",
    "def train(dataloader, model, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (cat, f, contact) in enumerate(dataloader):\n",
    "        cat, f = cat.float(), f.float()\n",
    "        cat, f, contact = cat.to(device), f.to(device), contact.to(device)\n",
    "        #f = f * 0.001\n",
    "        #print(f)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(cat)\n",
    "        pred = pred.squeeze(0)\n",
    "        #print(pred)\n",
    "        #loss = loss_fn(pred, f)\n",
    "        loss = contact_loss(pred, f, contact)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(cat)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    total_absolute_error, test_loss = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cat, f, contact in dataloader:\n",
    "            cat, f = cat.float(), f.float()\n",
    "            cat, f, contact = cat.to(device), f.to(device), contact.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predictions\n",
    "            pred = model(cat)\n",
    "            pred = pred.squeeze(0)  # Ensure correct shape if necessary\n",
    "            \n",
    "            # Accumulate Mean Squared Error loss (MSE) as used in training\n",
    "            test_loss += contact_loss(pred, f, contact).item()\n",
    "            \n",
    "            # Calculate Mean Absolute Error (MAE) for this batch\n",
    "            #total_absolute_error += torch.sum(torch.abs(pred - f)).item()\n",
    "            total_absolute_error += torch.sum(contact_loss(pred, f, contact, func='MAE')).item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_mse_loss = test_loss / num_batches\n",
    "    mae = total_absolute_error / size\n",
    "\n",
    "    print(f\"Test Results: \\n MAE: {mae:.5f}, Avg MSE Loss: {avg_mse_loss:.5f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d8411d7-0af3-43f2-8394-8b2edbad5dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.307230  [    1/   33]\n",
      "loss: 0.017039  [    2/   33]\n",
      "loss: 0.005657  [    3/   33]\n",
      "loss: 0.233500  [    4/   33]\n",
      "loss: 0.494319  [    5/   33]\n",
      "loss: 0.041492  [    6/   33]\n",
      "loss: 0.009757  [    7/   33]\n",
      "loss: 0.024964  [    8/   33]\n",
      "loss: 0.062175  [    9/   33]\n",
      "loss: 0.125688  [   10/   33]\n",
      "loss: 0.022470  [   11/   33]\n",
      "loss: 0.022873  [   12/   33]\n",
      "loss: 0.000522  [   13/   33]\n",
      "loss: 0.097092  [   14/   33]\n",
      "loss: 0.001549  [   15/   33]\n",
      "loss: 0.000032  [   16/   33]\n",
      "loss: 0.037148  [   17/   33]\n",
      "loss: 0.091034  [   18/   33]\n",
      "loss: 0.034727  [   19/   33]\n",
      "loss: 0.166097  [   20/   33]\n",
      "loss: 0.019003  [   21/   33]\n",
      "loss: 0.012995  [   22/   33]\n",
      "loss: 0.415239  [   23/   33]\n",
      "loss: 0.067076  [   24/   33]\n",
      "loss: 0.007780  [   25/   33]\n",
      "loss: 0.001712  [   26/   33]\n",
      "loss: 0.062856  [   27/   33]\n",
      "loss: 0.249283  [   28/   33]\n",
      "loss: 0.082959  [   29/   33]\n",
      "loss: 0.006768  [   30/   33]\n",
      "loss: 0.023765  [   31/   33]\n",
      "loss: 0.048874  [   32/   33]\n",
      "loss: 0.004599  [   33/   33]\n",
      "Test Results: \n",
      " MAE: 1.81709, Avg MSE Loss: 3.52861 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.321559  [    1/   33]\n",
      "loss: 0.039088  [    2/   33]\n",
      "loss: 0.032545  [    3/   33]\n",
      "loss: 0.190043  [    4/   33]\n",
      "loss: 0.428852  [    5/   33]\n",
      "loss: 0.018676  [    6/   33]\n",
      "loss: 0.000631  [    7/   33]\n",
      "loss: 0.006532  [    8/   33]\n",
      "loss: 0.049881  [    9/   33]\n",
      "loss: 0.107815  [   10/   33]\n",
      "loss: 0.034698  [   11/   33]\n",
      "loss: 0.048844  [   12/   33]\n",
      "loss: 0.005648  [   13/   33]\n",
      "loss: 0.107695  [   14/   33]\n",
      "loss: 0.000057  [   15/   33]\n",
      "loss: 0.000019  [   16/   33]\n",
      "loss: 0.018049  [   17/   33]\n",
      "loss: 0.001731  [   18/   33]\n",
      "loss: 0.063550  [   19/   33]\n",
      "loss: 0.156717  [   20/   33]\n",
      "loss: 0.030559  [   21/   33]\n",
      "loss: 0.006272  [   22/   33]\n",
      "loss: 0.419540  [   23/   33]\n",
      "loss: 0.074773  [   24/   33]\n",
      "loss: 0.015299  [   25/   33]\n",
      "loss: 0.000122  [   26/   33]\n",
      "loss: 0.047810  [   27/   33]\n",
      "loss: 0.254357  [   28/   33]\n",
      "loss: 0.058356  [   29/   33]\n",
      "loss: 0.002631  [   30/   33]\n",
      "loss: 0.065811  [   31/   33]\n",
      "loss: 0.058121  [   32/   33]\n",
      "loss: 0.001890  [   33/   33]\n",
      "Test Results: \n",
      " MAE: 1.77609, Avg MSE Loss: 3.37738 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.338327  [    1/   33]\n",
      "loss: 0.048907  [    2/   33]\n",
      "loss: 0.041881  [    3/   33]\n",
      "loss: 0.167013  [    4/   33]\n",
      "loss: 0.396312  [    5/   33]\n",
      "loss: 0.017737  [    6/   33]\n",
      "loss: 0.047191  [    7/   33]\n",
      "loss: 0.004131  [    8/   33]\n",
      "loss: 0.043446  [    9/   33]\n",
      "loss: 0.114250  [   10/   33]\n",
      "loss: 0.031526  [   11/   33]\n",
      "loss: 0.009053  [   12/   33]\n",
      "loss: 0.000637  [   13/   33]\n",
      "loss: 0.124429  [   14/   33]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#train(train_dataloader, model, loss_fn, optimizer)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#test(test_dataloader, model, loss_fn)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m test(test_dataloader, model)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, optimizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[9], line 71\u001b[0m, in \u001b[0;36mRPC_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     69\u001b[0m         pointcloud \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointcloud_transform(pointcloud)\n\u001b[0;32m     70\u001b[0m pointcloud_tensor \u001b[38;5;241m=\u001b[39m pointcloud\u001b[38;5;241m.\u001b[39mpos\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m---> 71\u001b[0m image, pointcloud_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, pointcloud_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m pointcloud_tensor \u001b[38;5;241m=\u001b[39m pointcloud_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     73\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\tv_tensors\\_tv_tensor.py:77\u001b[0m, in \u001b[0;36mTVTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Like in the base Tensor.__torch_function__ implementation, it's easier to always use\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# DisableTorchFunctionSubclass and then manually re-wrap the output if necessary\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DisableTorchFunctionSubclass():\n\u001b[1;32m---> 77\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m must_return_subclass \u001b[38;5;241m=\u001b[39m _must_return_subclass()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_return_subclass \u001b[38;5;129;01mor\u001b[39;00m (func \u001b[38;5;129;01min\u001b[39;00m _FORCE_TORCHFUNCTION_SUBCLASS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mcls\u001b[39m)):\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# If you're wondering why we need the `isinstance(args[0], cls)` check, remove it and see what fails\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# in test_to_tv_tensor_reference().\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# `args = (a_pure_tensor, an_image)` first. Without this guard, `out` would\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# be wrapped into an `Image`.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #train(train_dataloader, model, loss_fn, optimizer)\n",
    "    #test(test_dataloader, model, loss_fn)\n",
    "    train(train_dataloader, model, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "    if (t+1) % 2 == 0:\n",
    "        scheduler.step()\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e523a-c897-4716-a71e-0de047b47e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
