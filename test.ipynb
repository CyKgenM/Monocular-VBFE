{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3017e4-b9a3-4bce-a444-26d2a575a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a41e7-2f4b-4bde-af3a-6b7140cc160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare data\n",
    "rgb_file = 'data/train_images/frame_105.png'\n",
    "#depth_file = 'data/kitti_demo/depth/0000000050.png'\n",
    "intrinsic = [212.010, 212.010, 213.846, 121.795] #[707.0493, 707.0493, 604.0814, 180.5066]\n",
    "rgb_origin = cv2.imread(rgb_file)[:, :, ::-1]\n",
    "\n",
    "# Adjust input size to fit model requirements\n",
    "input_size = (616, 1064)  # For ViT model; use (544, 1216) for ConvNeXt model\n",
    "h, w = rgb_origin.shape[:2]\n",
    "scale = min(input_size[0] / h, input_size[1] / w)\n",
    "rgb = cv2.resize(rgb_origin, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Scale intrinsic parameters\n",
    "intrinsic = [intrinsic[0] * scale, intrinsic[1] * scale, intrinsic[2] * scale, intrinsic[3] * scale]\n",
    "\n",
    "# Pad the image to match model input size\n",
    "padding = [123.675, 116.28, 103.53]\n",
    "h, w = rgb.shape[:2]\n",
    "pad_h = input_size[0] - h\n",
    "pad_w = input_size[1] - w\n",
    "pad_h_half = pad_h // 2\n",
    "pad_w_half = pad_w // 2\n",
    "rgb = cv2.copyMakeBorder(rgb, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=padding)\n",
    "pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "# Normalize the image\n",
    "mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "rgb = torch.div((rgb - mean), std)\n",
    "rgb = rgb[None, :, :, :].cuda()\n",
    "\n",
    "# Load pre-trained model and perform inference\n",
    "model = torch.hub.load('yvanyin/metric3d', 'metric3d_vit_large', pretrain=True) # was 'metric3d_vit_small'\n",
    "model.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    pred_depth, confidence, output_dict = model.inference({'input': rgb})\n",
    "\n",
    "# Remove padding\n",
    "pred_depth = pred_depth.squeeze()\n",
    "pred_depth = pred_depth[pad_info[0]: pred_depth.shape[0] - pad_info[1], pad_info[2]: pred_depth.shape[1] - pad_info[3]]\n",
    "\n",
    "# Upsample to original size\n",
    "pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], rgb_origin.shape[:2], mode='bilinear').squeeze()\n",
    "print(pred_depth.size())\n",
    "# Convert depth to metric space (if needed)\n",
    "canonical_to_real_scale = intrinsic[0] / 1000.0  # Adjust based on focal length of canonical camera\n",
    "pred_depth = pred_depth * canonical_to_real_scale\n",
    "pred_depth = torch.clamp(pred_depth, 0, 300)  # Clamping depth values for visualization\n",
    "print(pred_depth.size())\n",
    "\n",
    "pointcloud = depth_to_pointcloud(pred_depth, intrinsic)\n",
    "print(\"Point cloud shape:\", pointcloud.shape)  # Expected shape [H*W, 3]\n",
    "'''\n",
    "# Save or use predicted depth\n",
    "pred_depth_np = pred_depth.cpu().numpy()\n",
    "cv2.imwrite('predicted_depth.png', (pred_depth_np * 255 / pred_depth_np.max()).astype(np.uint8))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c38e7a-be93-4f93-b6ef-174f4578c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87348801-be6d-409a-b69c-07a5344ee6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialBlock, self).__init__()\n",
    "        \n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        self.rgb_feature_extractor = nn.Sequential(*list(vgg.features.children()), \n",
    "                                                   nn.Flatten(), \n",
    "                                                   *list(vgg.classifier.children())[:-1])\n",
    "        \n",
    "        pointnet = PNet(k=40, normal_channel=False)\n",
    "        #self.pc_feature_extractor = nn.Sequential(*list(pointnet\n",
    "        self.pc_feature_extractor = pointnet\n",
    "\n",
    "    def forward(self, rgb_input, pc_input):\n",
    "        rgb_features = self.rgb_feature_extractor(rgb_input)\n",
    "        pc_features = self.pc_feature_extractor(pc_input)\n",
    "\n",
    "        combined_features = torch.cat([rgb_features, pc_features], dim=1)\n",
    "\n",
    "        output = F.relu(self.fc(combined_features))\n",
    "\n",
    "        return output\n",
    "\n",
    "SB = SpatialBlock().to(device)\n",
    "print(SB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bdad7-82ea-498e-94e4-615c1eea146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=3, kernel_size=3, dilation_base=2, output_dim=1):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        current_dilation = 1\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            conv = nn.Conv1d(input_dim, input_dim, kernel_size=kernel_size, padding=current_dilation, dilation=current_dilation)\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(input_dim))\n",
    "            current_dilation *= dilation_base  # Dilating each layer\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "        # Fully connected layer to output force prediction\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is expected to be of shape [batch_size, input_dim, sequence_length]\n",
    "        tcn_output = self.tcn(x)  # Temporal convolution over the sequence\n",
    "        tcn_output = tcn_output[:, :, tcn_output.size(2) // 2]  # Take the middle time step features\n",
    "        output = self.fc(tcn_output)  # Predict force from the middle step\n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "# Suppose we have a sequence of spatial features with shape [batch_size, input_dim, sequence_length]\n",
    "input_dim = 4608  # From the spatial block's output\n",
    "sequence_length = 15  # Number of frames in the temporal window\n",
    "\n",
    "temporal_block = TemporalBlock(input_dim=input_dim, num_layers=3, kernel_size=3)\n",
    "input_data = torch.randn(15, input_dim, sequence_length)  # Example input with batch size 8\n",
    "output = temporal_block(input_data)\n",
    "print(output.shape)  # Expected output shape: [8, 1]\n",
    "\n",
    "TB = TemporalBlock(input_dim=input_dim).to(device)\n",
    "print(TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8694682f-a490-4901-a7ac-f4a4dded1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26421880/26421880 [00:02<00:00, 10187010.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 451346.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4422102/4422102 [00:01<00:00, 3806350.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<00:00, 4627577.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.297380  [   64/60000]\n",
      "loss: 2.296994  [ 6464/60000]\n",
      "loss: 2.272621  [12864/60000]\n",
      "loss: 2.265488  [19264/60000]\n",
      "loss: 2.267900  [25664/60000]\n",
      "loss: 2.210917  [32064/60000]\n",
      "loss: 2.234223  [38464/60000]\n",
      "loss: 2.188991  [44864/60000]\n",
      "loss: 2.191795  [51264/60000]\n",
      "loss: 2.160941  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.160184 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.164182  [   64/60000]\n",
      "loss: 2.165017  [ 6464/60000]\n",
      "loss: 2.108556  [12864/60000]\n",
      "loss: 2.123044  [19264/60000]\n",
      "loss: 2.088042  [25664/60000]\n",
      "loss: 2.009572  [32064/60000]\n",
      "loss: 2.052499  [38464/60000]\n",
      "loss: 1.965186  [44864/60000]\n",
      "loss: 1.977815  [51264/60000]\n",
      "loss: 1.901420  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 1.907632 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.931223  [   64/60000]\n",
      "loss: 1.912678  [ 6464/60000]\n",
      "loss: 1.799432  [12864/60000]\n",
      "loss: 1.841545  [19264/60000]\n",
      "loss: 1.735130  [25664/60000]\n",
      "loss: 1.674700  [32064/60000]\n",
      "loss: 1.709755  [38464/60000]\n",
      "loss: 1.601662  [44864/60000]\n",
      "loss: 1.635921  [51264/60000]\n",
      "loss: 1.521114  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.545260 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.602072  [   64/60000]\n",
      "loss: 1.574153  [ 6464/60000]\n",
      "loss: 1.426857  [12864/60000]\n",
      "loss: 1.498888  [19264/60000]\n",
      "loss: 1.380584  [25664/60000]\n",
      "loss: 1.368632  [32064/60000]\n",
      "loss: 1.387130  [38464/60000]\n",
      "loss: 1.305843  [44864/60000]\n",
      "loss: 1.348254  [51264/60000]\n",
      "loss: 1.237299  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.269969 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.340276  [   64/60000]\n",
      "loss: 1.326344  [ 6464/60000]\n",
      "loss: 1.162984  [12864/60000]\n",
      "loss: 1.266187  [19264/60000]\n",
      "loss: 1.146265  [25664/60000]\n",
      "loss: 1.163502  [32064/60000]\n",
      "loss: 1.184841  [38464/60000]\n",
      "loss: 1.118219  [44864/60000]\n",
      "loss: 1.164441  [51264/60000]\n",
      "loss: 1.069814  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.096802 \n",
      "\n",
      "Done!\n",
      "Saved PyTorch Model State to model.pth\n",
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_17652\\1003079821.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b1173b-cb5d-48ee-a386-461fccff8708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 512])\n"
     ]
    }
   ],
   "source": [
    "from pointnet import PNet\n",
    "\n",
    "# Example usage\n",
    "batch_size = 15\n",
    "num_points = 2048\n",
    "pointcloud = torch.rand(batch_size, 3, num_points)  # Random point cloud data\n",
    "\n",
    "# Create PointNet model and run forward pass\n",
    "model = PNet(normal_channel=False)\n",
    "output = model(pointcloud)  # Output shape: (batch_size, num_classes)\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd70697-868b-4068-bfb5-90bbaafe69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet2 import PNetPP\n",
    "\n",
    "# Example usage\n",
    "batch_size = 15\n",
    "num_points = 2048\n",
    "pointcloud = torch.rand(batch_size, 3, num_points)  # Random point cloud data\n",
    "\n",
    "# Create PointNet model and run forward pass\n",
    "model = PNetPP(num_class=40, normal_channel=False)\n",
    "output = model(pointcloud)  # Output shape: (batch_size, num_classes)\n",
    "print(output[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
